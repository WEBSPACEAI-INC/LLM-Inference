{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Highly Optimized LLM Inference in Google Colab\n",
        "\n",
        "##### This notebook provides a highly optimized guide for performing efficient Large Language Model (LLM) inference using vLLM in Google Colab.\n"
      ],
      "metadata": {
        "id": "1aDl9UKINUR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## A. Setup Colab Environment & vLLM Installation\n",
        "\n",
        "# Check GPU availability and memory to guide optimization decisions.\n",
        "!nvidia-smi  # Displays GPU info; check memory to adjust settings.\n",
        "\n",
        "# Install vLLM and dependencies efficiently, ensuring the latest version for performance improvements.\n",
        "!pip install -q --upgrade vllm nest_asyncio\n",
        "\n",
        "# Import and apply nest_asyncio for Colab's asynchronous environment.\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "NE-Qgo5tNoPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## B. Hugging Face Authentication (Optional but Recommended)\n",
        "\n",
        "# Log in to Hugging Face for gated models or to avoid rate limits.\n",
        "# Uncomment and run the following lines as needed.\n",
        "#!pip install -q huggingface_hub\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "\n",
        "# Tip: Set HF_TOKEN environment variable for seamless access (uncomment and replace 'your_token_here').\n",
        "# import os\n",
        "# os.environ['HF_TOKEN'] = 'your_token_here'\n"
      ],
      "metadata": {
        "id": "llFg09nlNvGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## C. Model Selection\n",
        "\n",
        "# Choose a lightweight model optimized for Colab's free-tier GPU (T4, ~15GB VRAM).\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B parameters, efficient for Colab.\n",
        "\n",
        "# Alternatives for experimentation (uncomment as needed):\n",
        "# model_id = \"Qwen/Qwen1.5-1.8B-Chat\"  # 1.8B parameters, very lightweight.\n",
        "# model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"  # Quantized 7B, memory-efficient.\n",
        "\n",
        "# Note: Quantized models (e.g., AWQ) reduce memory usage and speed up inference.\n",
        "\n",
        "# ## C.1 Optional: Download the Model from Hugging Face\n",
        "\n",
        "# By default, vLLM will automatically download the model if it's not cached.\n",
        "# However, you can explicitly download the model for caching or to ensure availability.\n",
        "# Uncomment the following lines to download the model to a local directory.\n",
        "\n",
        "# from huggingface_hub import snapshot_download\n",
        "# local_dir = f\"/content/models/{model_id.split('/')[-1]}\"\n",
        "# snapshot_download(repo_id=model_id, local_dir=local_dir)\n",
        "# model_id = local_dir  # Set model_id to the local path after downloading\n",
        "\n",
        "# Note: For gated models, ensure you are logged in to Hugging Face (see section B).\n",
        "# Disk space: Models can be several GB; Colab provides ~100GB, but be mindful of multiple downloads"
      ],
      "metadata": {
        "id": "-b0sg9q9N-CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## D. Load the LLM Model with vLLM\n",
        "\n",
        "from vllm import LLM\n",
        "\n",
        "# Dynamic memory configuration based on Colab GPU.\n",
        "llm_args = {\n",
        "    \"model\": model_id,  # Will use Hugging Face ID or local path if downloaded.\n",
        "    \"tensor_parallel_size\": 1,  # Single GPU (default for free Colab).\n",
        "    \"trust_remote_code\": True,  # Required for some models.\n",
        "    \"dtype\": 'auto',  # 'float16' can save memory but may reduce precision.\n",
        "    \"gpu_memory_utilization\": 0.8,  # Conservative default to prevent OOM; adjust up to 0.95 if stable.\n",
        "    # \"max_model_len\": 2048,  # Uncomment to cap context length for memory savings.\n",
        "    # \"quantization\": \"awq\",  # Uncomment only for AWQ-quantized models.\n",
        "    \"enforce_eager\": False,  # Use CUDA graphs for faster execution (default: False).\n",
        "}\n",
        "\n",
        "# Load model with error handling and minimal resource overhead.\n",
        "llm = None\n",
        "try:\n",
        "    llm = LLM(**llm_args)\n",
        "except Exception as e:\n",
        "    print(f\"Model loading failed: {e}\")\n",
        "    print(\"Try: Smaller model, lower gpu_memory_utilization (e.g., 0.7), or set max_model_len=1024.\")"
      ],
      "metadata": {
        "id": "REPT4w1BOV8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## E. Define Sampling Parameters\n",
        "\n",
        "from vllm import SamplingParams\n",
        "\n",
        "# Flexible sampling parameters; tweak for task-specific needs.\n",
        "max_tokens = 100  # Reduced default to minimize memory usage; adjust as needed.\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6,  # Slightly lower for more focused outputs.\n",
        "    top_p=0.9,  # Nucleus sampling for efficiency.\n",
        "    max_tokens=max_tokens,\n",
        ")"
      ],
      "metadata": {
        "id": "eDAvZrwuPqkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## F. Perform Inference with Batch Control\n",
        "\n",
        "# Example prompts (editable by user).\n",
        "prompts = [\n",
        "    \"Summarize quantum mechanics briefly.\",\n",
        "    \"Write a haiku about machine learning.\",\n",
        "    \"List three advantages of vLLM.\",\n",
        "    \"Code a Python function for Fibonacci sequence.\"\n",
        "]\n",
        "\n",
        "# Batch size control to prevent OOM with large prompt sets.\n",
        "batch_size = 2  # Process prompts in smaller batches for memory efficiency.\n",
        "outputs = []\n",
        "\n",
        "if llm is not None:\n",
        "    try:\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            batch_prompts = prompts[i:i + batch_size]\n",
        "            batch_outputs = llm.generate(batch_prompts, sampling_params)\n",
        "            outputs.extend(batch_outputs)\n",
        "    except Exception as e:\n",
        "        print(f\"Inference failed: {e}\")\n",
        "        print(\"Reduce batch_size (e.g., 1) or max_tokens if OOM occurs.\")\n",
        "else:\n",
        "    print(\"Model not loaded; skipping inference.\")\n"
      ],
      "metadata": {
        "id": "x4AtV7qbPrSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## G. Display Results Efficiently\n",
        "\n",
        "if outputs:\n",
        "    for output in outputs:\n",
        "        prompt = output.prompt\n",
        "        text = output.outputs[0].text.strip()\n",
        "        print(f\"Prompt: {prompt}\\nResponse: {text}\\n{'-'*50}\")\n",
        "else:\n",
        "    print(\"No results to display.\")"
      ],
      "metadata": {
        "id": "vVWA0Pi_PvPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## H. (Optional) OpenAI-Compatible Server\n",
        "\n",
        "# Run an API server for advanced use cases (uncomment to enable).\n",
        "# Note: Consumes extra resources; use only if needed.\n",
        "# !nohup python -m vllm.entrypoints.openai.api_server --model {model_id} --gpu-memory-utilization 0.8 &\n",
        "\n",
        "# Example query (uncomment after installing openai: !pip install openai):\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n",
        "# response = client.completions.create(model=model_id, prompt=\"Hi!\", max_tokens=20)\n",
        "# print(response.choices[0].text.strip())"
      ],
      "metadata": {
        "id": "RJGm4zWcPyGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## I. Advanced Optimization Tips\n",
        "\n",
        "# - **Memory**: Use `max_model_len=1024` for very large models or low VRAM.\n",
        "# - **Speed**: Enable `enforce_eager=False` (default) for CUDA graph acceleration.\n",
        "# - **Batch Size**: Lower `batch_size` (e.g., 1) if VRAM is limited.\n",
        "# - **Quantization**: Add `\"quantization\": \"awq\"` for compatible models.\n",
        "# - **Runtime Reset**: Restart Colab runtime (!runtime > Restart runtime) for persistent issues.\n",
        "# - **Model Size**: Stick to <4B parameter models for free-tier reliability.\n",
        "\n",
        "# This notebook is highly optimized for speed, memory, and usability in Colab, with optional model downloading."
      ],
      "metadata": {
        "id": "LirPsW5zP2Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}