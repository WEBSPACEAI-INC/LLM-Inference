# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nGI7QV5PiOsyJhsxSIrtGJuW64bBSCEL

# Highly Optimized LLM Inference in Google Colab

##### This notebook provides a highly optimized guide for performing efficient Large Language Model (LLM) inference using vLLM in Google Colab.
"""

# ## A. Setup Colab Environment & vLLM Installation

# Check GPU availability and memory to guide optimization decisions.
!nvidia-smi  # Displays GPU info; check memory to adjust settings.

# Install vLLM and dependencies efficiently, ensuring the latest version for performance improvements.
!pip install -q --upgrade vllm nest_asyncio

# Import and apply nest_asyncio for Colab's asynchronous environment.
import nest_asyncio
nest_asyncio.apply()

# ## B. Hugging Face Authentication (Optional but Recommended)

# Log in to Hugging Face for gated models or to avoid rate limits.
# Uncomment and run the following lines as needed.
#!pip install -q huggingface_hub
# from huggingface_hub import notebook_login
# notebook_login()

# Tip: Set HF_TOKEN environment variable for seamless access (uncomment and replace 'your_token_here').
# import os
# os.environ['HF_TOKEN'] = 'your_token_here'

# ## C. Model Selection

# Choose a lightweight model optimized for Colab's free-tier GPU (T4, ~15GB VRAM).
model_id = "microsoft/Phi-3-mini-4k-instruct"  # 3.8B parameters, efficient for Colab.

# Alternatives for experimentation (uncomment as needed):
# model_id = "Qwen/Qwen1.5-1.8B-Chat"  # 1.8B parameters, very lightweight.
# model_id = "TheBloke/Mistral-7B-Instruct-v0.1-AWQ"  # Quantized 7B, memory-efficient.

# Note: Quantized models (e.g., AWQ) reduce memory usage and speed up inference.

# ## C.1 Optional: Download the Model from Hugging Face

# By default, vLLM will automatically download the model if it's not cached.
# However, you can explicitly download the model for caching or to ensure availability.
# Uncomment the following lines to download the model to a local directory.

# from huggingface_hub import snapshot_download
# local_dir = f"/content/models/{model_id.split('/')[-1]}"
# snapshot_download(repo_id=model_id, local_dir=local_dir)
# model_id = local_dir  # Set model_id to the local path after downloading

# Note: For gated models, ensure you are logged in to Hugging Face (see section B).
# Disk space: Models can be several GB; Colab provides ~100GB, but be mindful of multiple downloads

# ## D. Load the LLM Model with vLLM

from vllm import LLM

# Dynamic memory configuration based on Colab GPU.
llm_args = {
    "model": model_id,  # Will use Hugging Face ID or local path if downloaded.
    "tensor_parallel_size": 1,  # Single GPU (default for free Colab).
    "trust_remote_code": True,  # Required for some models.
    "dtype": 'auto',  # 'float16' can save memory but may reduce precision.
    "gpu_memory_utilization": 0.8,  # Conservative default to prevent OOM; adjust up to 0.95 if stable.
    # "max_model_len": 2048,  # Uncomment to cap context length for memory savings.
    # "quantization": "awq",  # Uncomment only for AWQ-quantized models.
    "enforce_eager": False,  # Use CUDA graphs for faster execution (default: False).
}

# Load model with error handling and minimal resource overhead.
llm = None
try:
    llm = LLM(**llm_args)
except Exception as e:
    print(f"Model loading failed: {e}")
    print("Try: Smaller model, lower gpu_memory_utilization (e.g., 0.7), or set max_model_len=1024.")

# ## E. Define Sampling Parameters

from vllm import SamplingParams

# Flexible sampling parameters; tweak for task-specific needs.
max_tokens = 100  # Reduced default to minimize memory usage; adjust as needed.
sampling_params = SamplingParams(
    temperature=0.6,  # Slightly lower for more focused outputs.
    top_p=0.9,  # Nucleus sampling for efficiency.
    max_tokens=max_tokens,
)

# ## F. Perform Inference with Batch Control

# Example prompts (editable by user).
prompts = [
    "Summarize quantum mechanics briefly.",
    "Write a haiku about machine learning.",
    "List three advantages of vLLM.",
    "Code a Python function for Fibonacci sequence."
]

# Batch size control to prevent OOM with large prompt sets.
batch_size = 2  # Process prompts in smaller batches for memory efficiency.
outputs = []

if llm is not None:
    try:
        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i + batch_size]
            batch_outputs = llm.generate(batch_prompts, sampling_params)
            outputs.extend(batch_outputs)
    except Exception as e:
        print(f"Inference failed: {e}")
        print("Reduce batch_size (e.g., 1) or max_tokens if OOM occurs.")
else:
    print("Model not loaded; skipping inference.")

# ## G. Display Results Efficiently

if outputs:
    for output in outputs:
        prompt = output.prompt
        text = output.outputs[0].text.strip()
        print(f"Prompt: {prompt}\nResponse: {text}\n{'-'*50}")
else:
    print("No results to display.")

# ## H. (Optional) OpenAI-Compatible Server

# Run an API server for advanced use cases (uncomment to enable).
# Note: Consumes extra resources; use only if needed.
# !nohup python -m vllm.entrypoints.openai.api_server --model {model_id} --gpu-memory-utilization 0.8 &

# Example query (uncomment after installing openai: !pip install openai):
# from openai import OpenAI
# client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")
# response = client.completions.create(model=model_id, prompt="Hi!", max_tokens=20)
# print(response.choices[0].text.strip())

# ## I. Advanced Optimization Tips

# - **Memory**: Use `max_model_len=1024` for very large models or low VRAM.
# - **Speed**: Enable `enforce_eager=False` (default) for CUDA graph acceleration.
# - **Batch Size**: Lower `batch_size` (e.g., 1) if VRAM is limited.
# - **Quantization**: Add `"quantization": "awq"` for compatible models.
# - **Runtime Reset**: Restart Colab runtime (!runtime > Restart runtime) for persistent issues.
# - **Model Size**: Stick to <4B parameter models for free-tier reliability.

# This notebook is highly optimized for speed, memory, and usability in Colab, with optional model downloading.